p2 <- qplot(cutWage,age,data=training,fill=cutWage, geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
?grid
?grid.arrange
install.packages("gridExtra")
library("gridExtra")
grid.arrange(p1,p2,ncol=2)
t1 <- table(cutWage,training$jobclass)
t1
prop.table(t1,1)
qplot(wage,colour=education,data=training,geom="density")
library(caret)
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type,p=0.75,list=F)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
hist(training$capitalAve,main="",xlab="ave. capital run length")
mean(training$capitalAve)
sd(training$capitalAve)
trainCapAve <- training$capitalAVE
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(tranCapAve)
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(tranCapAve)
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(treainCapAves)
mean(trainCapAves)
mean(trainCapAveS)
sd(trainCapAveS)
preObj <- preProcess(training[,58],method=c("center","scale"))
preObj <- preProcess(training[,-58],method=c("center","scale"))
preObj <- preProcess(training[,-58],method=c("center","scale"))$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
testCapAveS <- predict(preObj,testing[,-58])$capitalAve
testCapAveS <- predict(preObj,testing[,-58])$capitalAve
preObj
preObj <- preProcess(training[,-58],method=c("center","scale"))$capitalAve
preObj
preObj <- preProcess(training[,-58],method=c("center","scale"))
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
testCapAves <- predict(preObj,testing[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
mean(trainCapAves)
mean(testCapAves)
sd(testCapAves)
set.seed(32343)
modelFit <- train(type ~.,data=training, preProcess=c("center","scale"),method="glm")
modelFit
preObj <- preProcess(training[,-58],methodc("BoxCox"))
preObj <- preProcess(training[,-58],method=c("BoxCox"))
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
par(mfrow=c(1,2)); hist(trainCapAveS);qqnorm(trainCapAveS)
set.seed(13343)
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA] <- NA
preObj <- preProcess(training[,-58],method="knnImpute")
capAve <- predict(preObj,training[,-58])$capAve
install.packages("RANN",dependencies = T)
capAve <- predict(preObj,training[,-58])$capAve
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
quantile(capAve - capAveTruth)
quantile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[!selectNA])
library(kernlab);data(spam)
spam$capitalAveSq <- spam$capitalAve^2
library(ISLR)
library(caret)
data(Wage)
inTrain <- createDataPartition(y=Wage$wage,p=0.7,list=F)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
table(training$jobclass)
dummies <- dummyVars(wage ~ jobclass,data=training)
head(predict(dummies,newdata=training))
nsv <- nearZeroVar(training,saveMetrics=T)
nsv
library(splines)
bsBasis <- bs(training$age,df=3)
bsBasis
lm1 <- lm(wage ~ bsBasis, data=training)
plot(training$age,training$wage,pch=19,cex=0.5)
points(training$age,predict(lm1,newdata=training),col="red",pch=19,cex=0.5)
predict(bsBasis,age=testing$age)
library(caret);library(kernlab);data(spam)
inTrain <- createDataPartition(y=spam$type,p=0.75,list=F)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)
names(spam)[c(34,32)]
plot(spam[,34],spam[,32])
plot(spam[,34],spam[,32])
X <- 0.71*training$num415 + 0.71*training$num857
Y <- 0.71*training$num415 - 0.71*training$num857
plot(X,Y)
smallSpam <- spam[,c(34,32)]
prComp <- prcomp(smallSpam)
plot(prComp$x[,1],prComp$x[,2])
prComp$rotation
typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58] + 1))
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
spamPC <- predict(preProc,log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)
head(spam)
dim(spam)
ncol(spam)
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC<-predict(preProc,log10(training[,58]+1))
trainPC<-predict(preProc,log10(training[,-58]+1))
modelFit <- train(training$type ~ .,method="glm",data=trainPC)
testPC <- predict(preProc,log10(testing[,-58]+1))
confusionMatrix(testing$type,predict(modelFit,testPC))
modelFit <- train(training$type ~ ., method="glm",preProcess="pca",data=training)
confusionMatrix(testing$type,predict(modelFit,testing))
library(caret);data(faithful); set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting,p=0.5,list=FALS)
inTrain <- createDataPartition(y=faithful$waiting,p=0.5,list=FALSE)
trainFait <- faithful[inTrain,]; testFait <- faithful[-inTrain,]
head(trainFaith)
head(trainFait)
plot(trainFait$waiting,trainFait$eruptions,pch=19,col="blue",xlab="waiting",ylab="Duration")
lm1 <- lm(eruptions ~ waiting,data=trainFait)
summary(lm1)
lines(trainFait$waiting,lm$fitted,lwd=3)
lines(trainFait$waiting,lm1$fitted,lwd=3)
coef(lm1)[1] + coef(lm1)[2]*80
newdata <- data.frame(waiting=80)
predict(lm1,newdata)
newdata <- data.frame(waiting=120)
predict(lm1,newdata)
lm1
par(mfrow=C(1,2))
par(mfrow=c(1,2))
plot(trainFait$waiting,trainFait$eruptions,pch=19,col="blue",xlab="waiting",ylab="Duration")
lines(trainFait$waiting,predict(lm1),lwd=3)
plot(testFait$waiting,testFait$eruptions,pch=19,col="blue",xlab="waiting",ylab="Duration")
lines(testFait$waiting,predict(lm1,newdata=testFait),lwd=3)
sqrt(sum(lm1$fitted-trainFait$eruptions)^2)
sqrt(sum((lm1$fitted-trainFait$eruptions)^2))
sqrt(sum((predict(lm1,newdata=testFait)-trainFait$eruptions)^2))
sqrt(sum((predict(lm1,newdata=testFait)-testFait$eruptions)^2))
pred1 <- predict(lm1,newdata=testFaith,interval="prediction")
pred1 <- predict(lm1,newdata=testFait,interval="prediction")
ord <- order(testFait$waiting)
plot(testFait$waiting,testFait$eruptions,pch=19,col="blue")
matlines(testFait$waiting[ord],pred1[ord,],type="l",col=c(1,2,2),lty=c(1,1,1),lwd=3)
modFit <- train(eruptions ~ waiting, data=trainFait,method="lm")
summary(modFit$finalModel)
library(ISLR)
library(ggplot2)
library(caret)
data(wage)
data(Wage)
Wage <- subset(Wage,select=-c(logwage))
summary(Wage)
library(caret)
library(parallel)
library(doParallel)
#load raw data
training <- read.csv("pml-training.csv", header = TRUE)
test  <- read.csv('pml-testing.csv')
#remove columns with over a 90% of not a number
nasPerColumn<- apply(training,2,function(x) {sum(is.na(x))});
training <- training[,which(nasPerColumn <  nrow(training)*0.9)];
#remove near zero variance predictors
nearZeroColumns <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, nearZeroColumns$nzv==FALSE]
#remove not relevant columns for classification (x, user_name, raw time stamp 1  and 2, "new_window" and "num_window")
training<-training[,7:ncol(training)]
#class into factor
training$classe <- factor(training$classe)
trainIndex <- createDataPartition(y = training$classe, p=0.6,list=FALSE);
trainingPartition <- training[trainIndex,];
testingPartition <- training[-trainIndex,];
#random seed
set.seed(3433)
#parallel computing for multi-core
registerDoParallel(makeCluster(detectCores()))
#three models are generated:  random forest   ("rf"), boosted trees ("gbm") and linear discriminant analysis ("lda") model
model_rf <- train(classe ~ .,  method="rf", data=trainingPartition)
model_gbm <-train(classe ~ ., method = 'gbm', data = trainingPartition)
model_lda <-train(classe ~ ., method = 'lda', data = trainingPartition)
print("Random forest accuracy ")
rf_accuracy<- predict(model_rf, testingPartition)
print(confusionMatrix(rf_accuracy, testingPartition$classe))
print("Boosted trees accuracy ")
gbm_accuracy<- predict(model_gbm , testingPartition)
print(confusionMatrix(gbm_accuracy, testingPartition$classe))
print("Linear discriminant analysis")
lda_accuracy<- predict(model_lda , testingPartition)
print(confusionMatrix(lda_accuracy, testingPartition$classe))
#random seed
set.seed(3433)
#parallel computing for multi-core
registerDoParallel(makeCluster(detectCores()))
controlf <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
model_rf_CV <- train(classe ~ ., method="rf",  data=trainingPartition, trControl = controlf)
print("Random forest accuracy after CV")
rf_CV_accuracy<<- predict(model_rf_CV , testingPartition)
print(confusionMatrix(rf_CV_accuracy, testingPartition$classe))
library(ISLR)
library(ggplot2)
library(caret)
data(Wage); Wage <- subset(Wage,select=-c(logwage))
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot="pairs")
qplot(age,wage,data=training)
qplot(age,wage,colour=jobclassdata=training)
qplot(age,wage,colour=jobclass,data=training)
qplot(age,wage,colour=education,data=training)
modFit <- train(wage ~ age + jobclass + education, method = "lm", data = training)
finMod <- modFit$finalModel
print(modFit)
plot(finMod,1,pch=19,cex=0.5,col="#00000010")
qplot(finMod$fitted,finMod$residuals,colour=race,data=training)
plot(finMod$residuals,pch =19)
pred <- predict(modFit, testing)
qplot(Wage,pred,colour=year,data=testing)
qplot(wage,pred,colour=year,data=testing)
modFitAll <- train(wage ~ ., data=training, method="lm")
pred <- predict(modFitAll, testing)
qplot(wage,pred,data=testing)
data(iris)
library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training)
dim(testing)
qplot(Peta.Width,Sepal.Width,colour=Species,data=training)
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
library(caret)
modFit <- train(Species ~ . , method="rpart",data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=T, main="Classification Tree")
text(modFit$finalModel, use.n=T, all=T,cex=.8)
library(rattle)
install.packages("rattle")
fancyRpartPlot(modFit$finalModel)
library(rattle)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("rpart.plot", dependencies=T)
fancyRpartPlot(modFit$finalModel)
predict(modFit,newdata=testing)
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(ozone,package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)
ll <- matrix(NA,nrow=10,ncol=155)
for(i in 1:10){}
for(i in 1:10){
ss <- sample(1:dim(ozone)[1],replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}
head(ll)
plot(ozone$ozone,ozone$temperature,pch=10,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B=10, bagContro=bagControl(fit=ctreeBag$fit,predict=ctreeBag$pred,aggregate=ctreeBag$aggregate))
install.packages("party",dependencies=T)
treebag <- bag(predictors, temperature, B=10, bagContro=bagControl(fit=ctreeBag$fit,predict=ctreeBag$pred,aggregate=ctreeBag$aggregate))
plot(ozone$ozone,temperature,col='lightgrey',pch=19)
points(ozone$ozone,predict(treebag$fits[[1]]$fit,predictors),pch=19,col="red")
points(ozone$ozone,predict(treebag,predictors),pch=19,col="blue")
ctreeBag$fit
ctreeBag$pred
ctreeBag$aggregate
data(iris)
library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,p=0.7,list=FALSE)
library(caret)
inTrain <- createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
modFit <- train(Species~.,data=training,method="rf",prox=TRUE)
modFit
irisP <- classCenter(training[,c(3,4)],training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP)
irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species, data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
pred <- predict(modFit,testing)
testing$predRight <- pred==testing$Species
table(pred,testing$Species)
qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="newdata Predictions")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain <- createDataPartition(y=segmentationOriginal$Case,p=0.7,list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
dim(training)
dim(testing)
head(testing)
head(segmentationOriginal$Case)
inTrain <- grep("Train",segmentationOriginal$Case)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
fit <- train(Class~.,data=training,method="rpart")
fancyRpartPlot(fit$finalModel)
library(rattle)
fancyRpartPlot(fit$finalModel)
predData <- training[1:3,]
which(colnames(training)=="TotalIntenCh2")
which(colnames(training)=="FiberWidthCh1")
which(colnames(training)=="PerimStatusCh1")
predData[1,c(103,50,85)]=c(2300,10,2)
predData[2,c(103,50,85)]=c(50000,10,100)
predData[3,c(103,50,85)]=c(57000,8,100)
predict(fit,predData)
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
fit <- train(Area~.,data=olive,method="rpart")
pred <- predict(fit,newdata)
fancyRpartPlot(fit$finalModel)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
fit <- train(Area~.,data=olive,method="rpart")
pred <- predict(fit,newdata)
fancyRpartPlot(fit$finalModel)
pred
print(fit$finalModel)
unique(olive$Area)
head(olive$Area)
library(tseries)
install.packages("tseries")
library(tseries)
x <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
head(x)
?write
?write.csv
library(caret)
library(parallel)
library(doParallel)
library(tseries)
#raw data load step
if(!file.exists('pml-training.csv')){
training <- read.csv(url('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'))
write.csv(training,'pml-training.csv')
}else
training <- read.csv('pml-training.csv', header = TRUE)
if(!file.exists('pml-testing.csv')){
testing <- read.csv(url('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'))
write.csv(testing,'pml-testing.csv')
}else
testing  <- read.csv('pml-testing.csv')
#The columns with not number over a 90% removed
nasPerCol <- apply(training,2,function(x) {sum(is.na(x))});
training  <- training[,which(nasPerCol <  nrow(training)*0.9)];
#Near zero variance predictors removed
nearZeroCol <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, nearZeroCol$nzv==FALSE]
#Columns considered as not relevant for classification removed
#(x, user_name, raw time stamp 1  and 2, "new_window" and "num_window")
training<-training[,7:ncol(training)]
#class as factor
training$classe <- factor(training$classe)
trainIndex <- createDataPartition(y = training$classe, p=0.6,list=FALSE);
trainingPartition <- training[trainIndex,];
testingPartition <- training[-trainIndex,];
#random seed
set.seed(2423)
#parallel computing for multi-core
registerDoParallel(makeCluster(detectCores()))
#three models are generated:
#random forest   ("rf"),
model_rf <- train(classe ~ .,  method="rf", data=trainingPartition)
install.package("doParallel")
install.packages("doParallel")
library(caret)
library(parallel)
library(doParallel)
library(tseries)
#raw data load step
if(!file.exists('pml-training.csv')){
training <- read.csv(url('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'))
write.csv(training,'pml-training.csv')
}else
training <- read.csv('pml-training.csv', header = TRUE)
if(!file.exists('pml-testing.csv')){
testing <- read.csv(url('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'))
write.csv(testing,'pml-testing.csv')
}else
testing  <- read.csv('pml-testing.csv')
#The columns with not number over a 90% removed
nasPerCol <- apply(training,2,function(x) {sum(is.na(x))});
training  <- training[,which(nasPerCol <  nrow(training)*0.9)];
#Near zero variance predictors removed
nearZeroCol <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, nearZeroCol$nzv==FALSE]
#Columns considered as not relevant for classification removed
#(x, user_name, raw time stamp 1  and 2, "new_window" and "num_window")
training<-training[,7:ncol(training)]
#class as factor
training$classe <- factor(training$classe)
trainIndex <- createDataPartition(y = training$classe, p=0.6,list=FALSE);
trainingPartition <- training[trainIndex,];
testingPartition <- training[-trainIndex,];
#random seed
set.seed(2423)
#parallel computing for multi-core
registerDoParallel(makeCluster(detectCores()))
#three models are generated:
#random forest   ("rf"),
model_rf <- train(classe ~ .,  method="rf", data=trainingPartition)
#boosted trees ("gbm")
model_gbm <-train(classe ~ ., method = 'gbm', data = trainingPartition)
#linear discriminant analysis ("lda") model
model_lda <-train(classe ~ ., method = 'lda', data = trainingPartition)
print("Random forest accuracy ")
rf_accuracy<- predict(model_rf, testingPartition)
print(confusionMatrix(rf_accuracy, testingPartition$classe))
print("Boosted trees accuracy ")
gbm_accuracy<- predict(model_gbm , testingPartition)
print(confusionMatrix(gbm_accuracy, testingPartition$classe))
print("Linear discriminant analysis")
lda_accuracy<- predict(model_lda , testingPartition)
print(confusionMatrix(lda_accuracy, testingPartition$classe))
#random seed
set.seed(2423)
#parallel computing for multi-core
registerDoParallel(makeCluster(detectCores()))
controlf <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
model_rf_CV <- train(classe ~ ., method="rf",  data=trainingPartition, trControl = controlf)
print("Random forest accuracy after CV")
rf_CV_accuracy<<- predict(model_rf_CV , testingPartition)
print(confusionMatrix(rf_CV_accuracy, testingPartition$classe))
print("Variables importance at the model")
vi = varImp(model_rf_CV$finalModel)
vi$var<-rownames(vi)
vi = as.data.frame(vi[with(vi, order(vi$Overall, decreasing=TRUE)), ])
rownames(vi) <- NULL
print(vi)
pml_create_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
predictionassignmet<- function(){
prediction <- predict(model_rf_CV, test)
print(prediction)
answers <- as.vector(prediction)
pml_create_files(answers)
}
predictionassignmet()
predictionassignmet<- function(){
prediction <- predict(model_rf_CV, testing)
print(prediction)
answers <- as.vector(prediction)
pml_create_files(answers)
}
predictionassignmet()
install.packages('devtools')
library(devtools)
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='lrazo',
token='D1784A6759AAC475516E5F99460B9ABD',
secret='<SECRET>')
shinyapps::setAccountInfo(name='lrazo', token='D1784A6759AAC475516E5F99460B9ABD', secret='TcJUGNqa59F0KgIOmBG2bPPsGltQVNGflbMttOKQ')
library(shinyapps)
shiny::runApp('Shiny/Test7')
y
install_github('slidify','ramnathv')
install_github('slidifyLibraries','ramnathv')
setwd("C:\Users\Luis\Documents\Synthese2014\Synthesis2014")
setwd("C:\\Users\\Luis\\Documents\\Synthese2014\\Synthesis2014")
slidify("index.Rmd")
library(slidify)
slidify("index.Rmd")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
